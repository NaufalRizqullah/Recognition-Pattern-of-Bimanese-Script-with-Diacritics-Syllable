{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c683a3b-ada8-490d-80c8-431a9eaa98b1",
   "metadata": {},
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4286fb0-8450-4414-a69e-c45c3762cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209ad6f5-55c6-43b2-9eb4-cde7ad01e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1, and its: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: {}, and its: {}\".format(len(physical_devices), physical_devices) )\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecc945-34ca-4bba-8479-f41a9bedc5da",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afad16a4-7dab-4b18-bc6e-3586af60e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    #1. load\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    #2. preprocessing\n",
    "    X = data.drop(columns=['label'])\n",
    "    y = data['label'].values\n",
    "    \n",
    "    # robust scaler\n",
    "    scalerRobust = RobustScaler()\n",
    "    X = scalerRobust.fit_transform(X)\n",
    "    \n",
    "    # Encode to int label\n",
    "    labelencoder = LabelEncoder()\n",
    "    y = labelencoder.fit_transform(y)\n",
    "    # To categorical Label\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01399d24-be73-4701-b91d-f4e240e34089",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data('./13-01-2022_05-47-39_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0d969-560c-43e8-9431-619690b5125f",
   "metadata": {},
   "source": [
    "# Build KFold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b603a6-f095-432c-ac7a-68d012c44363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='sigmoid', input_dim=X.shape[1], name=\"hidden_layer_1\"))\n",
    "    model.add(Dense(256, activation='sigmoid', name=\"hidden_layer_2\"))\n",
    "    model.add(Dense(y.shape[1], activation='softmax', name='output'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = adam,\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e67bb70-a97d-4230-8096-929389f7a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18b9931-e008-4b64-9e39-cd23dd0df6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(n_split, X, y):\n",
    "    \n",
    "    kFold = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in kFold.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "        model = create_model()\n",
    "        model.fit(X_train, y_train, batch_size=64, epochs=500, verbose=0)\n",
    "        evaluated = model.evaluate(X_test, y_test)\n",
    "        print('Model evaluation ', evaluated)\n",
    "        \n",
    "        \n",
    "        avg_result.append([n_split, evaluated[0], evaluated[1]])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78e5f9-73a2-4a75-a2f8-6009e3174045",
   "metadata": {},
   "source": [
    "``` Click the button to reveal!\n",
    "y_pred = model.predict(xtest)\n",
    "ytest = np.argmax(ytest, axis=1)\n",
    "ypred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "print(\"Accuracy: {}\".format(accuracy_score(ytest, ypred)))\n",
    "print(\"Precision: {}\".format(precision_score(ytest, ypred, average='micro')))\n",
    "print(\"Recall: {}\".format(recall_score(ytest, ypred, average='micro')))\n",
    "print(\"F1-Score: {}\".format(f1_score(ytest, ypred, average='micro')))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a16be9d-962a-4153-9259-0a15f4f6a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 1s 8ms/step - loss: 2.8768 - accuracy: 0.6719\n",
      "Model evaluation  [2.876765012741089, 0.6718518733978271]\n",
      "127/127 [==============================] - 1s 9ms/step - loss: 2.8873 - accuracy: 0.6852\n",
      "Model evaluation  [2.8872811794281006, 0.6851851940155029]\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 2.1234 - accuracy: 0.7300\n",
      "Model evaluation  [2.1233997344970703, 0.7300000190734863]\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 2.4010 - accuracy: 0.7381\n",
      "Model evaluation  [2.401017189025879, 0.7381481528282166]\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 2.0670 - accuracy: 0.7530\n",
      "Model evaluation  [2.067049980163574, 0.7529629468917847]\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 2.0504 - accuracy: 0.7170\n",
      "Model evaluation  [2.0503976345062256, 0.7170370221138]\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 1.8016 - accuracy: 0.7630\n",
      "Model evaluation  [1.8016307353973389, 0.7629629373550415]\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 1.9743 - accuracy: 0.7432\n",
      "Model evaluation  [1.9742584228515625, 0.7432098984718323]\n",
      "64/64 [==============================] - 1s 10ms/step - loss: 2.0511 - accuracy: 0.7590\n",
      "Model evaluation  [2.05108642578125, 0.7590123414993286]\n",
      "51/51 [==============================] - 1s 9ms/step - loss: 2.0916 - accuracy: 0.7457\n",
      "Model evaluation  [2.091592311859131, 0.7456790208816528]\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5755 - accuracy: 0.77 - 1s 10ms/step - loss: 1.5563 - accuracy: 0.7772\n",
      "Model evaluation  [1.5562652349472046, 0.7771604657173157]\n",
      "51/51 [==============================] - 1s 9ms/step - loss: 1.8911 - accuracy: 0.7611\n",
      "Model evaluation  [1.8911265134811401, 0.7611111402511597]\n",
      "51/51 [==============================] - 1s 8ms/step - loss: 1.8356 - accuracy: 0.7667\n",
      "Model evaluation  [1.8355718851089478, 0.7666666507720947]\n",
      "51/51 [==============================] - 1s 8ms/step - loss: 1.8390 - accuracy: 0.7870\n",
      "Model evaluation  [1.8389885425567627, 0.7870370149612427]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.5682 - accuracy: 0.7519\n",
      "Model evaluation  [1.5681517124176025, 0.7518518567085266]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.5097 - accuracy: 0.7985\n",
      "Model evaluation  [1.5097110271453857, 0.7985185384750366]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.5440 - accuracy: 0.8007\n",
      "Model evaluation  [1.5440239906311035, 0.8007407188415527]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.9055 - accuracy: 0.7556\n",
      "Model evaluation  [1.9054971933364868, 0.7555555701255798]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.5736 - accuracy: 0.7874\n",
      "Model evaluation  [1.5735678672790527, 0.787407398223877]\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 1.7522 - accuracy: 0.7704\n",
      "Model evaluation  [1.752150058746338, 0.770370364189148]\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 1.7943 - accuracy: 0.7314\n",
      "Model evaluation  [1.7943357229232788, 0.7314335107803345]\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 1.7968 - accuracy: 0.7744\n",
      "Model evaluation  [1.7967779636383057, 0.7744165658950806]\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 1.4789 - accuracy: 0.7969\n",
      "Model evaluation  [1.4789409637451172, 0.796888530254364]\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 1.8834 - accuracy: 0.7736\n",
      "Model evaluation  [1.8834022283554077, 0.7735522985458374]\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6604 - accuracy: 0.79 - 1s 8ms/step - loss: 1.6279 - accuracy: 0.7952\n",
      "Model evaluation  [1.6278513669967651, 0.7951598763465881]\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 1.6604 - accuracy: 0.8021\n",
      "Model evaluation  [1.660427451133728, 0.8020743131637573]\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 1.7081 - accuracy: 0.7934\n",
      "Model evaluation  [1.7080724239349365, 0.793431282043457]\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 1.7845 - accuracy: 0.7710\n",
      "Model evaluation  [1.7845368385314941, 0.7709773182868958]\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 1.6626 - accuracy: 0.7947\n",
      "Model evaluation  [1.6626354455947876, 0.7946692705154419]\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.4189 - accuracy: 0.7966\n",
      "Model evaluation  [1.4188992977142334, 0.7966436147689819]\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 1.6795 - accuracy: 0.7670\n",
      "Model evaluation  [1.6795467138290405, 0.7670286297798157]\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 1.8907 - accuracy: 0.7826\n",
      "Model evaluation  [1.8906702995300293, 0.782608687877655]\n",
      "32/32 [==============================] - 1s 11ms/step - loss: 2.0379 - accuracy: 0.7599\n",
      "Model evaluation  [2.0378706455230713, 0.7598814368247986]\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.6024 - accuracy: 0.7757\n",
      "Model evaluation  [1.6023579835891724, 0.7756916880607605]\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.6913 - accuracy: 0.7955\n",
      "Model evaluation  [1.691264271736145, 0.7954545617103577]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.6967 - accuracy: 0.7833\n",
      "Model evaluation  [1.696662187576294, 0.7833333611488342]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.6992 - accuracy: 0.7744\n",
      "Model evaluation  [1.6991618871688843, 0.7744444608688354]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.6277 - accuracy: 0.8100\n",
      "Model evaluation  [1.6276776790618896, 0.8100000023841858]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4146 - accuracy: 0.8144\n",
      "Model evaluation  [1.4145686626434326, 0.8144444227218628]\n",
      "29/29 [==============================] - 1s 9ms/step - loss: 2.0264 - accuracy: 0.7978\n",
      "Model evaluation  [2.0264246463775635, 0.7977777719497681]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.5729 - accuracy: 0.7911\n",
      "Model evaluation  [1.5728806257247925, 0.7911111116409302]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4436 - accuracy: 0.8133\n",
      "Model evaluation  [1.443564772605896, 0.8133333325386047]\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2099 - accuracy: 0.8100\n",
      "Model evaluation  [1.2098745107650757, 0.8100000023841858]\n",
      "29/29 [==============================] - 1s 10ms/step - loss: 1.8555 - accuracy: 0.7822\n",
      "Model evaluation  [1.8554725646972656, 0.7822222113609314]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.4609 - accuracy: 0.7802\n",
      "Model evaluation  [1.4608838558197021, 0.780246913433075]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.0172 - accuracy: 0.7728\n",
      "Model evaluation  [2.0171730518341064, 0.7728394865989685]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.6099 - accuracy: 0.7889\n",
      "Model evaluation  [1.6099214553833008, 0.7888888716697693]\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3664 - accuracy: 0.8062\n",
      "Model evaluation  [1.366437554359436, 0.8061728477478027]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.6308 - accuracy: 0.7827\n",
      "Model evaluation  [1.6308109760284424, 0.7827160358428955]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.9056 - accuracy: 0.7790\n",
      "Model evaluation  [1.9056315422058105, 0.7790123224258423]\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.6724 - accuracy: 0.7889\n",
      "Model evaluation  [1.6724413633346558, 0.7888888716697693]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.6257 - accuracy: 0.8099\n",
      "Model evaluation  [1.625677466392517, 0.809876561164856]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.3635 - accuracy: 0.7877\n",
      "Model evaluation  [1.3634830713272095, 0.7876543402671814]\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.6790 - accuracy: 0.8148\n",
      "Model evaluation  [1.678960919380188, 0.8148148059844971]\n",
      "Wall time: 8h 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(2, 11):\n",
    "    kfold(n, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5475ab44-92da-4999-988e-38ff282b741a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2.876765012741089, 0.6718518733978271],\n",
       " [2, 2.8872811794281006, 0.6851851940155029],\n",
       " [3, 2.1233997344970703, 0.7300000190734863],\n",
       " [3, 2.401017189025879, 0.7381481528282166],\n",
       " [3, 2.067049980163574, 0.7529629468917847],\n",
       " [4, 2.0503976345062256, 0.7170370221138],\n",
       " [4, 1.8016307353973389, 0.7629629373550415],\n",
       " [4, 1.9742584228515625, 0.7432098984718323],\n",
       " [4, 2.05108642578125, 0.7590123414993286],\n",
       " [5, 2.091592311859131, 0.7456790208816528],\n",
       " [5, 1.5562652349472046, 0.7771604657173157],\n",
       " [5, 1.8911265134811401, 0.7611111402511597],\n",
       " [5, 1.8355718851089478, 0.7666666507720947],\n",
       " [5, 1.8389885425567627, 0.7870370149612427],\n",
       " [6, 1.5681517124176025, 0.7518518567085266],\n",
       " [6, 1.5097110271453857, 0.7985185384750366],\n",
       " [6, 1.5440239906311035, 0.8007407188415527],\n",
       " [6, 1.9054971933364868, 0.7555555701255798],\n",
       " [6, 1.5735678672790527, 0.787407398223877],\n",
       " [6, 1.752150058746338, 0.770370364189148],\n",
       " [7, 1.7943357229232788, 0.7314335107803345],\n",
       " [7, 1.7967779636383057, 0.7744165658950806],\n",
       " [7, 1.4789409637451172, 0.796888530254364],\n",
       " [7, 1.8834022283554077, 0.7735522985458374],\n",
       " [7, 1.6278513669967651, 0.7951598763465881],\n",
       " [7, 1.660427451133728, 0.8020743131637573],\n",
       " [7, 1.7080724239349365, 0.793431282043457],\n",
       " [8, 1.7845368385314941, 0.7709773182868958],\n",
       " [8, 1.6626354455947876, 0.7946692705154419],\n",
       " [8, 1.4188992977142334, 0.7966436147689819],\n",
       " [8, 1.6795467138290405, 0.7670286297798157],\n",
       " [8, 1.8906702995300293, 0.782608687877655],\n",
       " [8, 2.0378706455230713, 0.7598814368247986],\n",
       " [8, 1.6023579835891724, 0.7756916880607605],\n",
       " [8, 1.691264271736145, 0.7954545617103577],\n",
       " [9, 1.696662187576294, 0.7833333611488342],\n",
       " [9, 1.6991618871688843, 0.7744444608688354],\n",
       " [9, 1.6276776790618896, 0.8100000023841858],\n",
       " [9, 1.4145686626434326, 0.8144444227218628],\n",
       " [9, 2.0264246463775635, 0.7977777719497681],\n",
       " [9, 1.5728806257247925, 0.7911111116409302],\n",
       " [9, 1.443564772605896, 0.8133333325386047],\n",
       " [9, 1.2098745107650757, 0.8100000023841858],\n",
       " [9, 1.8554725646972656, 0.7822222113609314],\n",
       " [10, 1.4608838558197021, 0.780246913433075],\n",
       " [10, 2.0171730518341064, 0.7728394865989685],\n",
       " [10, 1.6099214553833008, 0.7888888716697693],\n",
       " [10, 1.366437554359436, 0.8061728477478027],\n",
       " [10, 1.6308109760284424, 0.7827160358428955],\n",
       " [10, 1.9056315422058105, 0.7790123224258423],\n",
       " [10, 1.6724413633346558, 0.7888888716697693],\n",
       " [10, 1.625677466392517, 0.809876561164856],\n",
       " [10, 1.3634830713272095, 0.7876543402671814],\n",
       " [10, 1.678960919380188, 0.8148148059844971]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdc5acb-f0a4-44b9-b77e-18979cfc9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(avg_result, columns=[\"K\", \"Loss\", \"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7db861-e7a1-4ab2-a94a-7823bc785a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('kfold_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a14e9d-e2fb-4783-a5b0-9403e09fa5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.876765</td>\n",
       "      <td>0.671852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.887281</td>\n",
       "      <td>0.685185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.123400</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.401017</td>\n",
       "      <td>0.738148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2.067050</td>\n",
       "      <td>0.752963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2.050398</td>\n",
       "      <td>0.717037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1.801631</td>\n",
       "      <td>0.762963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1.974258</td>\n",
       "      <td>0.743210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2.051086</td>\n",
       "      <td>0.759012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2.091592</td>\n",
       "      <td>0.745679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1.556265</td>\n",
       "      <td>0.777160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1.891127</td>\n",
       "      <td>0.761111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>1.835572</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>1.838989</td>\n",
       "      <td>0.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>1.568152</td>\n",
       "      <td>0.751852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>1.509711</td>\n",
       "      <td>0.798519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>1.544024</td>\n",
       "      <td>0.800741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>1.905497</td>\n",
       "      <td>0.755556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>1.573568</td>\n",
       "      <td>0.787407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>1.752150</td>\n",
       "      <td>0.770370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>1.794336</td>\n",
       "      <td>0.731434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>1.796778</td>\n",
       "      <td>0.774417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>1.478941</td>\n",
       "      <td>0.796889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>1.883402</td>\n",
       "      <td>0.773552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>1.627851</td>\n",
       "      <td>0.795160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>1.660427</td>\n",
       "      <td>0.802074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>1.708072</td>\n",
       "      <td>0.793431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>1.784537</td>\n",
       "      <td>0.770977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>1.662635</td>\n",
       "      <td>0.794669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>1.418899</td>\n",
       "      <td>0.796644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>1.679547</td>\n",
       "      <td>0.767029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>1.890670</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>2.037871</td>\n",
       "      <td>0.759881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8</td>\n",
       "      <td>1.602358</td>\n",
       "      <td>0.775692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>1.691264</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>1.696662</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>1.699162</td>\n",
       "      <td>0.774444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9</td>\n",
       "      <td>1.627678</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>1.414569</td>\n",
       "      <td>0.814444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>2.026425</td>\n",
       "      <td>0.797778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9</td>\n",
       "      <td>1.572881</td>\n",
       "      <td>0.791111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>1.443565</td>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>9</td>\n",
       "      <td>1.209875</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9</td>\n",
       "      <td>1.855473</td>\n",
       "      <td>0.782222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10</td>\n",
       "      <td>1.460884</td>\n",
       "      <td>0.780247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10</td>\n",
       "      <td>2.017173</td>\n",
       "      <td>0.772839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>1.609921</td>\n",
       "      <td>0.788889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>1.366438</td>\n",
       "      <td>0.806173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>1.630811</td>\n",
       "      <td>0.782716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>1.905632</td>\n",
       "      <td>0.779012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>1.672441</td>\n",
       "      <td>0.788889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10</td>\n",
       "      <td>1.625677</td>\n",
       "      <td>0.809877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10</td>\n",
       "      <td>1.363483</td>\n",
       "      <td>0.787654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10</td>\n",
       "      <td>1.678961</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     K      Loss  Accuracy\n",
       "0    2  2.876765  0.671852\n",
       "1    2  2.887281  0.685185\n",
       "2    3  2.123400  0.730000\n",
       "3    3  2.401017  0.738148\n",
       "4    3  2.067050  0.752963\n",
       "5    4  2.050398  0.717037\n",
       "6    4  1.801631  0.762963\n",
       "7    4  1.974258  0.743210\n",
       "8    4  2.051086  0.759012\n",
       "9    5  2.091592  0.745679\n",
       "10   5  1.556265  0.777160\n",
       "11   5  1.891127  0.761111\n",
       "12   5  1.835572  0.766667\n",
       "13   5  1.838989  0.787037\n",
       "14   6  1.568152  0.751852\n",
       "15   6  1.509711  0.798519\n",
       "16   6  1.544024  0.800741\n",
       "17   6  1.905497  0.755556\n",
       "18   6  1.573568  0.787407\n",
       "19   6  1.752150  0.770370\n",
       "20   7  1.794336  0.731434\n",
       "21   7  1.796778  0.774417\n",
       "22   7  1.478941  0.796889\n",
       "23   7  1.883402  0.773552\n",
       "24   7  1.627851  0.795160\n",
       "25   7  1.660427  0.802074\n",
       "26   7  1.708072  0.793431\n",
       "27   8  1.784537  0.770977\n",
       "28   8  1.662635  0.794669\n",
       "29   8  1.418899  0.796644\n",
       "30   8  1.679547  0.767029\n",
       "31   8  1.890670  0.782609\n",
       "32   8  2.037871  0.759881\n",
       "33   8  1.602358  0.775692\n",
       "34   8  1.691264  0.795455\n",
       "35   9  1.696662  0.783333\n",
       "36   9  1.699162  0.774444\n",
       "37   9  1.627678  0.810000\n",
       "38   9  1.414569  0.814444\n",
       "39   9  2.026425  0.797778\n",
       "40   9  1.572881  0.791111\n",
       "41   9  1.443565  0.813333\n",
       "42   9  1.209875  0.810000\n",
       "43   9  1.855473  0.782222\n",
       "44  10  1.460884  0.780247\n",
       "45  10  2.017173  0.772839\n",
       "46  10  1.609921  0.788889\n",
       "47  10  1.366438  0.806173\n",
       "48  10  1.630811  0.782716\n",
       "49  10  1.905632  0.779012\n",
       "50  10  1.672441  0.788889\n",
       "51  10  1.625677  0.809877\n",
       "52  10  1.363483  0.787654\n",
       "53  10  1.678961  0.814815"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4c5cc-0133-418f-b264-292d258ceb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
